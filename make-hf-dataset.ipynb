{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets lightning tables\n",
    "!pip install -qq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple, Literal\n",
    "import pandas as pd\n",
    "from pandas.core.groupby import DataFrameGroupBy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Value,\n",
    "    Array2D,\n",
    "    DatasetInfo,\n",
    "    load_dataset,\n",
    "    ClassLabel,\n",
    "    Sequence,\n",
    ")\n",
    "from lightning.pytorch import LightningDataModule\n",
    "\n",
    "\n",
    "data_dir = Path(\"sqa_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and assembling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(pd_data, test_list):\n",
    "    data_select_index_test = pd_data[\"context_source_file\"] == \"000\"\n",
    "\n",
    "    for test_file_i in test_list:\n",
    "        data_select_index_test = (pd_data[\"context_source_file\"] == test_file_i) | (\n",
    "            data_select_index_test\n",
    "        )\n",
    "\n",
    "    print(\"Testing data percentage: \", sum(data_select_index_test) / pd_data.shape[0])\n",
    "    print(\"Testing data number: \", sum(data_select_index_test))\n",
    "    print(\n",
    "        \"Testing data unique scene: \",\n",
    "        len(pd_data[data_select_index_test].context_index.unique()),\n",
    "    )\n",
    "\n",
    "    return data_select_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading generated questions pickle\n",
    "win_len = 1800\n",
    "base_file_name = f\"s1234_{win_len}_600\"  # stride 600 is the one from the paper\n",
    "pd_data = pd.read_pickle(data_dir / f\"{base_file_name}_balanced.pkl\")\n",
    "with open(data_dir / f\"{base_file_name}_context.pkl\", \"rb\") as f:\n",
    "    sensory_data = pickle.load(f)\n",
    "\n",
    "# The OppQA data is split into a training set and a testing set. The\n",
    "# training set contains SQA data generated on the first two Activity-\n",
    "# of-Daily-Living (ADL) runs and a drill run of users 1-4, and the\n",
    "# rest of the runs are used to generate testing data.\n",
    "\n",
    "### splitting method 1: based on context\n",
    "valid_list = [\n",
    "    \"S1-ADL1.dat\",\n",
    "    \"S2-ADL1.dat\",\n",
    "    \"S3-ADL1.dat\",\n",
    "    \"S4-ADL1.dat\",\n",
    "    \"S1-ADL3.dat\",\n",
    "    \"S2-ADL3.dat\",\n",
    "    \"S3-ADL3.dat\",\n",
    "    \"S4-ADL3.dat\",\n",
    "    \"S1-ADL2.dat\",\n",
    "    \"S2-ADL2.dat\",\n",
    "    \"S3-ADL2.dat\",\n",
    "    \"S4-ADL2.dat\",\n",
    "]\n",
    "\n",
    "train_list = [\n",
    "    \"S1-ADL4.dat\",\n",
    "    \"S2-ADL4.dat\",\n",
    "    \"S3-ADL4.dat\",\n",
    "    \"S4-ADL4.dat\",\n",
    "    \"S1-ADL5.dat\",\n",
    "    \"S2-ADL5.dat\",\n",
    "    \"S3-ADL5.dat\",\n",
    "    \"S4-ADL5.dat\",\n",
    "    \"S1-Drill.dat\",\n",
    "    \"S2-Drill.dat\",\n",
    "    \"S3-Drill.dat\",\n",
    "    \"S4-Drill.dat\",\n",
    "]\n",
    "\n",
    "#     ============  split train/valid based on no overlapping context:  ============\n",
    "train_ind = dataset_split(pd_data, train_list)\n",
    "valid_ind = dataset_split(pd_data, valid_list)\n",
    "\n",
    "\n",
    "### splitting method 2: total random\n",
    "# ============ random split train/valid:  ============\n",
    "#     random_ind = np.random.rand(pd_data.shape[0])\n",
    "#     train_ind = random_ind>=0.8\n",
    "#     valid_ind = ~train_ind\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "#     ### splitting method 3: based on q_struct\n",
    "#     uniq_struct = ( pd_data.question_structure.unique() )\n",
    "#     print('Total unique Q structure num: ',  len(uniq_struct))\n",
    "#     # split the unique Q-struct to 50%-50%\n",
    "#     rd_num = np.random.rand(len(uniq_struct))\n",
    "#     train_ind_struct = rd_num<0.8\n",
    "#     test_ind_struct = rd_num>=0.8\n",
    "\n",
    "#     train_qstruct = uniq_struct[train_ind_struct]\n",
    "#     # valid_qstruct = uniq_struct[valid_ind]\n",
    "#     test_qstruct = uniq_struct[test_ind_struct]\n",
    "#     train_ind = pd_data.question_structure.isin(train_qstruct)\n",
    "#     valid_ind = pd_data.question_structure.isin(test_qstruct)\n",
    "\n",
    "#     print('Train/test split:  %d / %d' %(sum(train_ind), sum(valid_ind)) )\n",
    "#     # ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation is really the test set, i.e. the hold-out for final evaluation.\n",
    "test_ind = valid_ind\n",
    "del valid_ind\n",
    "\n",
    "# Generate a proper validation set\n",
    "percent_valid = 0.1\n",
    "\n",
    "# train_ind is effectively a mask, and we want to set some to false and have a copy where those are then true for the validation set\n",
    "# tghe selection shall be random\n",
    "valid_ind = np.zeros_like(train_ind)\n",
    "valid_ind[\n",
    "    np.random.choice(\n",
    "        np.where(train_ind)[0], int(percent_valid * np.sum(train_ind)), replace=False\n",
    "    )\n",
    "] = True\n",
    "train_ind[valid_ind] = False\n",
    "\n",
    "train_ind.sum(), valid_ind.sum(), test_ind.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data[\"answer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_multiclass = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data.iloc[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensory_data[\"raw\"][\"S1-ADL1.dat_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_key_list = (\n",
    "    pd_data[\"context_source_file\"] + \"_\" + pd_data[\"context_start_point\"].astype(str)\n",
    ")\n",
    "sensory_matrix = np.zeros((len(pd_data), win_len, 77), dtype=\"float32\")\n",
    "\n",
    "for key, values in tqdm(sensory_data[\"raw\"].items()):\n",
    "    sensory_matrix[np.where(context_key_list == key), :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample(NamedTuple):\n",
    "    # sample_id: int\n",
    "    question_id: int\n",
    "    trajectory: torch.Tensor\n",
    "    # textual_description: str\n",
    "    question_type: str\n",
    "    question: str\n",
    "    answer_type: str\n",
    "    answer: str\n",
    "    # options: dict[str, str | bool] | None\n",
    "    # correct_option: str\n",
    "\n",
    "\n",
    "def answer_type(answer: str) -> str:\n",
    "    if answer in (\"Yes\", \"No\"):\n",
    "        return \"binary\"\n",
    "    try:\n",
    "        int(answer)\n",
    "        return \"count\"  # This basically ranges from 0 to 3, each inclusive; could also be treated as \"open\"\n",
    "    except ValueError:\n",
    "        return \"multi\"\n",
    "\n",
    "\n",
    "def get_for(data: pd.DataFrame) -> list[Sample]:\n",
    "    return [\n",
    "        Sample(\n",
    "            # sample_id=num,\n",
    "            question_id=row[\"question_index\"],\n",
    "            trajectory=sensory_matrix[0, ...],\n",
    "            # textual_description=data[\"textual_description\"],\n",
    "            question_type=str(row[\"question_family_index\"]),\n",
    "            question=row[\"question\"],\n",
    "            answer_type=answer_type(row[\"answer\"]),\n",
    "            answer=row[\"answer\"],\n",
    "            # options=json.dumps(qa_pair[\"options\"]),  # can be None\n",
    "            # correct_option=qa_pair[\"correct_option\"],\n",
    "        )\n",
    "        for _, row in data.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "get_for(pd_data.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame.from_dict(get_for(pd_data))\n",
    "df_all[\"question-answer\"] = df_all[\"question\"] + \" \" + df_all[\"answer\"]\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"question\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"answer_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"trajectory-str\"] = df_all[\"trajectory\"].apply(lambda x: str(x))\n",
    "df_all[\n",
    "    [\n",
    "        \"question_type\",\n",
    "        \"answer_type\",\n",
    "        \"answer\",\n",
    "        \"question\",\n",
    "        \"question-answer\",\n",
    "        \"trajectory-str\",\n",
    "    ]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_shape = next(iter(get_for(pd_data.iloc[:1]))).trajectory.shape\n",
    "trajectory_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_options = pd.Series(\n",
    "    pd_data[~pd_data[\"answer\"].isin((\"Yes\", \"No\", \"0\", \"1\", \"2\", \"3\"))][\n",
    "        \"answer\"\n",
    "    ].unique()\n",
    ").sort_values()\n",
    "multi_option_to_int = {answer: index for index, answer in enumerate(multi_options)}\n",
    "multi_option_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info(task: Literal[\"binary\", \"open\", \"multi\", \"count\"]) -> DatasetInfo:\n",
    "    base_features = {\n",
    "        # \"sample_id\": Value(\"int32\"),\n",
    "        \"question_id\": Value(\"int32\"),\n",
    "        \"trajectory\": Array2D(dtype=\"float32\", shape=trajectory_shape),\n",
    "        # \"textual_description\": Value(\"string\"),\n",
    "        \"question_type\": Value(\"string\"),\n",
    "        \"question\": Value(\"string\"),\n",
    "        \"answer_type\": Value(\"string\"),\n",
    "        # \"answer\": Value(\"string\"),\n",
    "        # \"options\": Value(\"string\"),  # JSON encoded\n",
    "        # \"correct_option\": Value(\"string\"),\n",
    "    }\n",
    "\n",
    "    match task:\n",
    "        case \"binary\":\n",
    "            answer_features = {\n",
    "                \"answer\": ClassLabel(names=[\"true\", \"false\"], num_classes=2)\n",
    "            }\n",
    "        case \"multi\":\n",
    "            answer_features = {\n",
    "                \"answer\": ClassLabel(\n",
    "                    names=multi_options.to_list(), num_classes=num_multiclass\n",
    "                ),\n",
    "                \"options\": Sequence(Value(\"string\")),\n",
    "            }\n",
    "        case \"count\":\n",
    "            answer_features = {\"answer\": Value(\"uint8\")}\n",
    "        case \"open\":\n",
    "            answer_features = {\"answer\": Value(\"string\")}\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid task '{task}'\")\n",
    "\n",
    "    return DatasetInfo(features=Features(base_features | answer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_grouped_df_to_hub(\n",
    "    df_group: DataFrameGroupBy,\n",
    "    split: Literal[\"test\", \"val\", \"train\"],\n",
    "    limit_task: list[Literal[\"open\", \"multi\", \"binary\"]] | None = None,\n",
    "    token: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a grouped DataFrame, feature types, dataset info, and a Hugging Face authentication token,\n",
    "    then pushes each group to the Hugging Face Hub under specified configurations.\n",
    "\n",
    "    :param df_group: Grouped Pandas DataFrame object.\n",
    "    :param feat_type: Feature type for the dataset.\n",
    "    :param info: Information about the dataset.\n",
    "    :param token: Hugging Face authentication token.\n",
    "    \"\"\"\n",
    "    for name, group in df_group:\n",
    "        if limit_task and name not in limit_task:\n",
    "            continue\n",
    "        print(f\"Group Name: {name} of split: {split}\")\n",
    "\n",
    "        match name:\n",
    "            case \"binary\":\n",
    "                group[\"answer\"] = (group[\"answer\"] == \"Yes\").astype(int)\n",
    "\n",
    "            case \"multi\":\n",
    "                # group[\"answer_index\"] = [\n",
    "                #     multi_option_to_int[ans] for ans in group[\"answer\"]\n",
    "                # ]\n",
    "                group[\"options\"] = group[\"answer\"].apply(\n",
    "                    lambda _: multi_options.to_list()\n",
    "                )\n",
    "\n",
    "            case \"count\":\n",
    "                group[\"answer\"] = group[\"answer\"].astype(int)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid task name: {name}\")\n",
    "\n",
    "        def gen_it():\n",
    "            yield from group.to_dict(orient=\"records\")\n",
    "\n",
    "        info = make_info(name)\n",
    "\n",
    "        match win_len:\n",
    "            case 500:\n",
    "                repo_name = \"dasyd/OppQA-500\"\n",
    "            case 1800:\n",
    "                repo_name = \"dasyd/OppQA\"\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    f\"This window length has no dataset attached: {win_len}\"\n",
    "                )\n",
    "\n",
    "        # Create a dataset from the list of dictionaries and push it to the hub\n",
    "        dataset = Dataset.from_generator(\n",
    "            gen_it,\n",
    "            info=info,  # , gen_kwargs=dict(split=split)\n",
    "        )\n",
    "        dataset.push_to_hub(\n",
    "            repo_name,\n",
    "            config_name=name,\n",
    "            token=token,\n",
    "            split=split,\n",
    "            #     commit_message=f\"[Version Revision] Restructured item shape of {split} split of {name} task dataset\",\n",
    "        )\n",
    "        print(f\"Pushed {name} of split: {split} to Hub {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": train_ind,\n",
    "    \"val\": valid_ind,\n",
    "    \"test\": test_ind,\n",
    "}\n",
    "\n",
    "for name, mask in splits.items():\n",
    "    data = get_for(pd_data[mask])\n",
    "    df = pd.DataFrame(elem._asdict() for elem in data)\n",
    "    push_grouped_df_to_hub(\n",
    "        df.groupby(\"answer_type\"),\n",
    "        split=name,\n",
    "        # limit_task=[\"binary\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to first run:\n",
    "\n",
    "```shell\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if it works (this re-downloads the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import VerificationMode\n",
    "\n",
    "\n",
    "class TimeQADataModule(LightningDataModule):\n",
    "    KEY = \"dasyd/OppQA\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        task: Literal[\"binary\", \"multi\", \"open\"] = \"multi\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.task = task\n",
    "\n",
    "    def _load_dataset_split(self, splits: list[str]):\n",
    "        \"\"\"Workaround to overcome the missing hf implementation of only dowloading the split shards\"\"\"\n",
    "\n",
    "        return load_dataset(\n",
    "            TimeQADataModule.KEY,\n",
    "            self.task,\n",
    "            data_dir=self.task,\n",
    "            data_files={split: f\"{split}-*\" for split in splits},\n",
    "            verification_mode=VerificationMode.NO_CHECKS,\n",
    "            num_proc=len(splits),\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        self._load_dataset_split([\"val\", \"train\"])\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.dataset = self._load_dataset_split([\"train\", \"val\"])\n",
    "        elif stage == \"test\":\n",
    "            self.dataset = self._load_dataset_split([\"test\"])\n",
    "\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.dataset[\"train\"], batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"val\"], batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"test\"], batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "module = TimeQADataModule(batch_size=4, task=\"multi\")\n",
    "module.prepare_data()\n",
    "module.setup(\"fit\")\n",
    "# module.setup(\"fit\")\n",
    "# module.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = module.train_dataloader()\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# list(batch.keys())\n",
    "batch[\"trajectory\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
