{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets huggingface_hub[cli] lightning tables\n",
    "!pip install -qq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple, Literal\n",
    "import pandas as pd\n",
    "from pandas.core.groupby import DataFrameGroupBy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Value,\n",
    "    Array2D,\n",
    "    DatasetInfo,\n",
    "    load_dataset,\n",
    "    ClassLabel,\n",
    "    Sequence,\n",
    ")\n",
    "from lightning.pytorch import LightningDataModule\n",
    "\n",
    "\n",
    "data_dir = Path(\"sqa_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and assembling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(pd_data, test_list):\n",
    "    data_select_index_test = pd_data[\"context_source_file\"] == \"000\"\n",
    "\n",
    "    for test_file_i in test_list:\n",
    "        data_select_index_test = (pd_data[\"context_source_file\"] == test_file_i) | (\n",
    "            data_select_index_test\n",
    "        )\n",
    "\n",
    "    print(\"Testing data percentage: \", sum(data_select_index_test) / pd_data.shape[0])\n",
    "    print(\"Testing data number: \", sum(data_select_index_test))\n",
    "    print(\n",
    "        \"Testing data unique scene: \",\n",
    "        len(pd_data[data_select_index_test].context_index.unique()),\n",
    "    )\n",
    "\n",
    "    return data_select_index_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing data percentage:  0.8283266370433441\n",
      "Testing data number:  93393\n",
      "Testing data unique scene:  729\n",
      "Testing data percentage:  0.17167336295665594\n",
      "Testing data number:  19356\n",
      "Testing data unique scene:  629\n"
     ]
    }
   ],
   "source": [
    "# loading generated questions pickle\n",
    "win_len = 1800\n",
    "base_file_name = f\"s1234_{win_len}_600\"  # stride 600 is the one from the paper\n",
    "pd_data = pd.read_pickle(data_dir / f\"{base_file_name}_balanced.pkl\")\n",
    "with open(data_dir / f\"{base_file_name}_context.pkl\", \"rb\") as f:\n",
    "    sensory_data = pickle.load(f)\n",
    "\n",
    "# The OppQA data is split into a training set and a testing set. The\n",
    "# training set contains SQA data generated on the first two Activity-\n",
    "# of-Daily-Living (ADL) runs and a drill run of users 1-4, and the\n",
    "# rest of the runs are used to generate testing data.\n",
    "\n",
    "### splitting method 1: based on context\n",
    "valid_list = [\n",
    "    \"S1-ADL1.dat\",\n",
    "    \"S2-ADL1.dat\",\n",
    "    \"S3-ADL1.dat\",\n",
    "    \"S4-ADL1.dat\",\n",
    "    \"S1-ADL3.dat\",\n",
    "    \"S2-ADL3.dat\",\n",
    "    \"S3-ADL3.dat\",\n",
    "    \"S4-ADL3.dat\",\n",
    "    \"S1-ADL2.dat\",\n",
    "    \"S2-ADL2.dat\",\n",
    "    \"S3-ADL2.dat\",\n",
    "    \"S4-ADL2.dat\",\n",
    "]\n",
    "\n",
    "train_list = [\n",
    "    \"S1-ADL4.dat\",\n",
    "    \"S2-ADL4.dat\",\n",
    "    \"S3-ADL4.dat\",\n",
    "    \"S4-ADL4.dat\",\n",
    "    \"S1-ADL5.dat\",\n",
    "    \"S2-ADL5.dat\",\n",
    "    \"S3-ADL5.dat\",\n",
    "    \"S4-ADL5.dat\",\n",
    "    \"S1-Drill.dat\",\n",
    "    \"S2-Drill.dat\",\n",
    "    \"S3-Drill.dat\",\n",
    "    \"S4-Drill.dat\",\n",
    "]\n",
    "\n",
    "#     ============  split train/valid based on no overlapping context:  ============\n",
    "train_ind = dataset_split(pd_data, train_list)\n",
    "valid_ind = dataset_split(pd_data, valid_list)\n",
    "\n",
    "\n",
    "### splitting method 2: total random\n",
    "# ============ random split train/valid:  ============\n",
    "#     random_ind = np.random.rand(pd_data.shape[0])\n",
    "#     train_ind = random_ind>=0.8\n",
    "#     valid_ind = ~train_ind\n",
    "\n",
    "# ====================================================\n",
    "\n",
    "#     ### splitting method 3: based on q_struct\n",
    "#     uniq_struct = ( pd_data.question_structure.unique() )\n",
    "#     print('Total unique Q structure num: ',  len(uniq_struct))\n",
    "#     # split the unique Q-struct to 50%-50%\n",
    "#     rd_num = np.random.rand(len(uniq_struct))\n",
    "#     train_ind_struct = rd_num<0.8\n",
    "#     test_ind_struct = rd_num>=0.8\n",
    "\n",
    "#     train_qstruct = uniq_struct[train_ind_struct]\n",
    "#     # valid_qstruct = uniq_struct[valid_ind]\n",
    "#     test_qstruct = uniq_struct[test_ind_struct]\n",
    "#     train_ind = pd_data.question_structure.isin(train_qstruct)\n",
    "#     valid_ind = pd_data.question_structure.isin(test_qstruct)\n",
    "\n",
    "#     print('Train/test split:  %d / %d' %(sum(train_ind), sum(valid_ind)) )\n",
    "#     # ===================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127089      True\n",
       "1164649    False\n",
       "1246654     True\n",
       "1404904     True\n",
       "810542      True\n",
       "           ...  \n",
       "1569687     True\n",
       "1572900     True\n",
       "1572908     True\n",
       "1572909     True\n",
       "1572917     True\n",
       "Name: context_source_file, Length: 112749, dtype: bool"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84054, 9339, 19356)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The validation is really the test set, i.e. the hold-out for final evaluation.\n",
    "test_ind = valid_ind\n",
    "del valid_ind\n",
    "\n",
    "# Generate a proper validation set\n",
    "percent_valid = 0.1\n",
    "\n",
    "# train_ind is effectively a mask, and we want to set some to false and have a copy where those are then true for the validation set\n",
    "# tghe selection shall be random\n",
    "valid_ind = np.zeros_like(train_ind)\n",
    "valid_ind[\n",
    "    np.random.choice(\n",
    "        np.where(train_ind)[0], int(percent_valid * np.sum(train_ind)), replace=False\n",
    "    )\n",
    "] = True\n",
    "train_ind[valid_ind] = False\n",
    "\n",
    "train_ind.sum(), valid_ind.sum(), test_ind.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_source_file</th>\n",
       "      <th>context_start_point</th>\n",
       "      <th>context_index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>question_family_index</th>\n",
       "      <th>question_structure</th>\n",
       "      <th>question_index</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127089</th>\n",
       "      <td>S1-ADL5.dat</td>\n",
       "      <td>9000</td>\n",
       "      <td>252</td>\n",
       "      <td>The tester closed the back Door After closing ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>3</td>\n",
       "      <td>3_['Close the back Door', 'Close the Fridge', ...</td>\n",
       "      <td>127089</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164649</th>\n",
       "      <td>S4-ADL1.dat</td>\n",
       "      <td>34200</td>\n",
       "      <td>1118</td>\n",
       "      <td>Is it true that the user opened the front Door...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>3_['Open the front Door', 'Close the Fridge', ...</td>\n",
       "      <td>1164649</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246654</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>3000</td>\n",
       "      <td>1294</td>\n",
       "      <td>The person opened the Fridge Following closing...</td>\n",
       "      <td>No</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>3</td>\n",
       "      <td>3_['Open the Fridge', 'Close the back Door', '...</td>\n",
       "      <td>1246654</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404904</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>24600</td>\n",
       "      <td>1330</td>\n",
       "      <td>The person closed the front Door After closing...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>3_['Close the front Door', 'Close the back Doo...</td>\n",
       "      <td>1404904</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810542</th>\n",
       "      <td>S2-Drill.dat</td>\n",
       "      <td>41400</td>\n",
       "      <td>705</td>\n",
       "      <td>The tester opened the back Door After opening ...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>3_['Open the back Door', 'Open the Fridge', 'O...</td>\n",
       "      <td>810542</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569687</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>42000</td>\n",
       "      <td>1359</td>\n",
       "      <td>Confirm if the user performs the same action F...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>9</td>\n",
       "      <td>9_['Close the front Door', 'Close the back Doo...</td>\n",
       "      <td>1569687</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572900</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>42600</td>\n",
       "      <td>1360</td>\n",
       "      <td>The subject performs the same action Preceding...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>9</td>\n",
       "      <td>9_['Open the back Door', 'Open the front Door'...</td>\n",
       "      <td>1572900</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572908</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>42600</td>\n",
       "      <td>1360</td>\n",
       "      <td>Is it the case that the subject performs the s...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>9</td>\n",
       "      <td>9_['Close the back Door', 'Close the front Doo...</td>\n",
       "      <td>1572908</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572909</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>42600</td>\n",
       "      <td>1360</td>\n",
       "      <td>The subject performs the same action Following...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>9</td>\n",
       "      <td>9_['Open the front Door', 'Open the back Door'...</td>\n",
       "      <td>1572909</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572917</th>\n",
       "      <td>S4-Drill.dat</td>\n",
       "      <td>42600</td>\n",
       "      <td>1360</td>\n",
       "      <td>The tester performs the same action Following ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Invalid</td>\n",
       "      <td>9</td>\n",
       "      <td>9_['Close the front Door', 'Close the back Doo...</td>\n",
       "      <td>1572917</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112749 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        context_source_file  context_start_point  context_index  \\\n",
       "127089          S1-ADL5.dat                 9000            252   \n",
       "1164649         S4-ADL1.dat                34200           1118   \n",
       "1246654        S4-Drill.dat                 3000           1294   \n",
       "1404904        S4-Drill.dat                24600           1330   \n",
       "810542         S2-Drill.dat                41400            705   \n",
       "...                     ...                  ...            ...   \n",
       "1569687        S4-Drill.dat                42000           1359   \n",
       "1572900        S4-Drill.dat                42600           1360   \n",
       "1572908        S4-Drill.dat                42600           1360   \n",
       "1572909        S4-Drill.dat                42600           1360   \n",
       "1572917        S4-Drill.dat                42600           1360   \n",
       "\n",
       "                                                  question answer pred_answer  \\\n",
       "127089   The tester closed the back Door After closing ...     No     Invalid   \n",
       "1164649  Is it true that the user opened the front Door...     No          No   \n",
       "1246654  The person opened the Fridge Following closing...     No     Invalid   \n",
       "1404904  The person closed the front Door After closing...     No         Yes   \n",
       "810542   The tester opened the back Door After opening ...     No         Yes   \n",
       "...                                                    ...    ...         ...   \n",
       "1569687  Confirm if the user performs the same action F...    Yes     Invalid   \n",
       "1572900  The subject performs the same action Preceding...    Yes     Invalid   \n",
       "1572908  Is it the case that the subject performs the s...    Yes     Invalid   \n",
       "1572909  The subject performs the same action Following...    Yes     Invalid   \n",
       "1572917  The tester performs the same action Following ...    Yes     Invalid   \n",
       "\n",
       "         question_family_index  \\\n",
       "127089                       3   \n",
       "1164649                      3   \n",
       "1246654                      3   \n",
       "1404904                      3   \n",
       "810542                       3   \n",
       "...                        ...   \n",
       "1569687                      9   \n",
       "1572900                      9   \n",
       "1572908                      9   \n",
       "1572909                      9   \n",
       "1572917                      9   \n",
       "\n",
       "                                        question_structure  question_index  \\\n",
       "127089   3_['Close the back Door', 'Close the Fridge', ...          127089   \n",
       "1164649  3_['Open the front Door', 'Close the Fridge', ...         1164649   \n",
       "1246654  3_['Open the Fridge', 'Close the back Door', '...         1246654   \n",
       "1404904  3_['Close the front Door', 'Close the back Doo...         1404904   \n",
       "810542   3_['Open the back Door', 'Open the Fridge', 'O...          810542   \n",
       "...                                                    ...             ...   \n",
       "1569687  9_['Close the front Door', 'Close the back Doo...         1569687   \n",
       "1572900  9_['Open the back Door', 'Open the front Door'...         1572900   \n",
       "1572908  9_['Close the back Door', 'Close the front Doo...         1572908   \n",
       "1572909  9_['Open the front Door', 'Open the back Door'...         1572909   \n",
       "1572917  9_['Close the front Door', 'Close the back Doo...         1572917   \n",
       "\n",
       "        split  \n",
       "127089   Test  \n",
       "1164649  Test  \n",
       "1246654  Test  \n",
       "1404904  Test  \n",
       "810542   Test  \n",
       "...       ...  \n",
       "1569687  Test  \n",
       "1572900  Test  \n",
       "1572908  Test  \n",
       "1572909  Test  \n",
       "1572917  Test  \n",
       "\n",
       "[112749 rows x 10 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 112749 entries, 127089 to 1572917\n",
      "Data columns (total 10 columns):\n",
      " #   Column                 Non-Null Count   Dtype \n",
      "---  ------                 --------------   ----- \n",
      " 0   context_source_file    112749 non-null  object\n",
      " 1   context_start_point    112749 non-null  int64 \n",
      " 2   context_index          112749 non-null  int64 \n",
      " 3   question               112749 non-null  object\n",
      " 4   answer                 112749 non-null  object\n",
      " 5   pred_answer            112749 non-null  object\n",
      " 6   question_family_index  112749 non-null  int64 \n",
      " 7   question_structure     112749 non-null  object\n",
      " 8   question_index         112749 non-null  int64 \n",
      " 9   split                  112749 non-null  object\n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 9.5+ MB\n"
     ]
    }
   ],
   "source": [
    "pd_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "No                         33361\n",
       "Yes                        29649\n",
       "1                          20650\n",
       "0                          20343\n",
       "Close the Fridge             771\n",
       "Open the front Door          755\n",
       "Open the back Door           729\n",
       "Close the back Door          710\n",
       "Toggle the Switch            696\n",
       "Close the front Door         691\n",
       "Open the Fridge              685\n",
       "Drink from the Cup           681\n",
       "Close the third Drawer       649\n",
       "Open the Dishwasher          630\n",
       "2                            345\n",
       "Open the third Drawer        270\n",
       "Clean the Table              251\n",
       "Close the Dishwasher         246\n",
       "Open the first Drawer        223\n",
       "Close the second Drawer      167\n",
       "Close the first Drawer       151\n",
       "Open the second Drawer        88\n",
       "3                              8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data[\"answer\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd_data[\"answer\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_multiclass = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context_source_file                                           S2-Drill.dat\n",
       "context_start_point                                                  18000\n",
       "context_index                                                          666\n",
       "question                 Is it the case that the user closed the front ...\n",
       "answer                                                                  No\n",
       "pred_answer                                                             No\n",
       "question_family_index                                                    3\n",
       "question_structure       3_['Close the front Door', 'Open the Fridge', ...\n",
       "question_index                                                      652310\n",
       "split                                                                 Test\n",
       "Name: 652310, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.iloc[15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['raw', 'embedding'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 77)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensory_data[\"raw\"][\"S1-ADL1.dat_0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [00:54<00:00, 25.18it/s]\n"
     ]
    }
   ],
   "source": [
    "context_key_list = (\n",
    "    pd_data[\"context_source_file\"] + \"_\" + pd_data[\"context_start_point\"].astype(str)\n",
    ")\n",
    "sensory_matrix = np.zeros((len(pd_data), win_len, 77), dtype=\"float32\")\n",
    "\n",
    "for key, values in tqdm(sensory_data[\"raw\"].items()):\n",
    "    sensory_matrix[np.where(context_key_list == key), :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'existence',\n",
       " 1: 'counting',\n",
       " 2: 'action_compare',\n",
       " 3: 'action_compare',\n",
       " 4: 'counting',\n",
       " 5: 'counting',\n",
       " 6: 'action_query',\n",
       " 7: 'action_query',\n",
       " 8: 'existence',\n",
       " 9: 'action_compare',\n",
       " 10: 'number_compare',\n",
       " 11: 'number_compare',\n",
       " 12: 'action_query',\n",
       " 13: 'counting',\n",
       " 14: 'time_query',\n",
       " 15: 'time_query'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(Path(\"sqa_data_gen\") / \"question_family.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "question_family_index_to_type = {\n",
    "    int(entry[\"index\"]): entry[\"question_type\"] for entry in data[\"questions\"]\n",
    "}\n",
    "question_family_index_to_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sample(question_id=127089, trajectory=array([[-1.103, -0.458,  0.174, ...,  0.442, -0.037, -0.174],\n",
       "        [-0.919, -0.351,  0.1  , ...,  1.22 ,  0.183, -0.172],\n",
       "        [-0.719, -0.302,  0.079, ...,  1.762,  0.37 , -0.17 ],\n",
       "        ...,\n",
       "        [-0.948, -0.315,  0.009, ..., -0.007,  0.036, -0.007],\n",
       "        [-0.946, -0.319,  0.013, ...,  0.047, -0.014, -0.007],\n",
       "        [-0.945, -0.317,  0.011, ...,  0.056, -0.005, -0.008]],\n",
       "       dtype=float32), question_family_index=3, question_type='action_compare', question='The tester closed the back Door After closing the refrigerator OR Before closing the front Door?', answer='No'),\n",
       " Sample(question_id=1164649, trajectory=array([[-1.103, -0.458,  0.174, ...,  0.442, -0.037, -0.174],\n",
       "        [-0.919, -0.351,  0.1  , ...,  1.22 ,  0.183, -0.172],\n",
       "        [-0.719, -0.302,  0.079, ...,  1.762,  0.37 , -0.17 ],\n",
       "        ...,\n",
       "        [-0.948, -0.315,  0.009, ..., -0.007,  0.036, -0.007],\n",
       "        [-0.946, -0.319,  0.013, ...,  0.047, -0.014, -0.007],\n",
       "        [-0.945, -0.317,  0.011, ...,  0.056, -0.005, -0.008]],\n",
       "       dtype=float32), question_family_index=3, question_type='action_compare', question='Is it true that the user opened the front Door Preceding closing the Fridge OR After opening the Fridge?', answer='No')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Sample(NamedTuple):\n",
    "    # sample_id: int\n",
    "    question_id: int\n",
    "    trajectory: torch.Tensor\n",
    "    # textual_description: str\n",
    "    question_family_index: int\n",
    "    question_type: str\n",
    "    question: str\n",
    "    # answer_type: str\n",
    "    answer: str\n",
    "    # options: dict[str, str | bool] | None\n",
    "    # correct_option: str\n",
    "\n",
    "\n",
    "def get_for(data: pd.DataFrame) -> list[Sample]:\n",
    "    return [\n",
    "        Sample(\n",
    "            # sample_id=num,\n",
    "            question_id=row[\"question_index\"],\n",
    "            trajectory=sensory_matrix[0, ...],\n",
    "            # textual_description=data[\"textual_description\"],\n",
    "            question_family_index=int(row[\"question_family_index\"]),\n",
    "            question_type=question_family_index_to_type[\n",
    "                int(row[\"question_family_index\"])\n",
    "            ],\n",
    "            question=row[\"question\"],\n",
    "            answer=row[\"answer\"],\n",
    "            # options=json.dumps(qa_pair[\"options\"]),  # can be None\n",
    "            # correct_option=qa_pair[\"correct_option\"],\n",
    "        )\n",
    "        for _, row in data.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "get_for(pd_data.iloc[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>trajectory</th>\n",
       "      <th>question_family_index</th>\n",
       "      <th>question_type</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>question-answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127089</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>3</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The tester closed the back Door After closing ...</td>\n",
       "      <td>No</td>\n",
       "      <td>The tester closed the back Door After closing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1164649</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>3</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>Is it true that the user opened the front Door...</td>\n",
       "      <td>No</td>\n",
       "      <td>Is it true that the user opened the front Door...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1246654</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>3</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The person opened the Fridge Following closing...</td>\n",
       "      <td>No</td>\n",
       "      <td>The person opened the Fridge Following closing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1404904</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>3</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The person closed the front Door After closing...</td>\n",
       "      <td>No</td>\n",
       "      <td>The person closed the front Door After closing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>810542</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>3</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The tester opened the back Door After opening ...</td>\n",
       "      <td>No</td>\n",
       "      <td>The tester opened the back Door After opening ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112744</th>\n",
       "      <td>1569687</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>9</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>Confirm if the user performs the same action F...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Confirm if the user performs the same action F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112745</th>\n",
       "      <td>1572900</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>9</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The subject performs the same action Preceding...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The subject performs the same action Preceding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112746</th>\n",
       "      <td>1572908</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>9</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>Is it the case that the subject performs the s...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Is it the case that the subject performs the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112747</th>\n",
       "      <td>1572909</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>9</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The subject performs the same action Following...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The subject performs the same action Following...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112748</th>\n",
       "      <td>1572917</td>\n",
       "      <td>[[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...</td>\n",
       "      <td>9</td>\n",
       "      <td>action_compare</td>\n",
       "      <td>The tester performs the same action Following ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>The tester performs the same action Following ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112749 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        question_id                                         trajectory  \\\n",
       "0            127089  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "1           1164649  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "2           1246654  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "3           1404904  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "4            810542  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "...             ...                                                ...   \n",
       "112744      1569687  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "112745      1572900  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "112746      1572908  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "112747      1572909  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "112748      1572917  [[-1.103, -0.458, 0.174, -1.914, -0.546, 0.091...   \n",
       "\n",
       "        question_family_index   question_type  \\\n",
       "0                           3  action_compare   \n",
       "1                           3  action_compare   \n",
       "2                           3  action_compare   \n",
       "3                           3  action_compare   \n",
       "4                           3  action_compare   \n",
       "...                       ...             ...   \n",
       "112744                      9  action_compare   \n",
       "112745                      9  action_compare   \n",
       "112746                      9  action_compare   \n",
       "112747                      9  action_compare   \n",
       "112748                      9  action_compare   \n",
       "\n",
       "                                                 question answer  \\\n",
       "0       The tester closed the back Door After closing ...     No   \n",
       "1       Is it true that the user opened the front Door...     No   \n",
       "2       The person opened the Fridge Following closing...     No   \n",
       "3       The person closed the front Door After closing...     No   \n",
       "4       The tester opened the back Door After opening ...     No   \n",
       "...                                                   ...    ...   \n",
       "112744  Confirm if the user performs the same action F...    Yes   \n",
       "112745  The subject performs the same action Preceding...    Yes   \n",
       "112746  Is it the case that the subject performs the s...    Yes   \n",
       "112747  The subject performs the same action Following...    Yes   \n",
       "112748  The tester performs the same action Following ...    Yes   \n",
       "\n",
       "                                          question-answer  \n",
       "0       The tester closed the back Door After closing ...  \n",
       "1       Is it true that the user opened the front Door...  \n",
       "2       The person opened the Fridge Following closing...  \n",
       "3       The person closed the front Door After closing...  \n",
       "4       The tester opened the back Door After opening ...  \n",
       "...                                                   ...  \n",
       "112744  Confirm if the user performs the same action F...  \n",
       "112745  The subject performs the same action Preceding...  \n",
       "112746  Is it the case that the subject performs the s...  \n",
       "112747  The subject performs the same action Following...  \n",
       "112748  The tester performs the same action Following ...  \n",
       "\n",
       "[112749 rows x 7 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.DataFrame.from_dict(get_for(pd_data))\n",
    "df_all[\"question-answer\"] = df_all[\"question\"] + \" \" + df_all[\"answer\"]\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_family_index\n",
       "0       503\n",
       "1       505\n",
       "2      2345\n",
       "3     38289\n",
       "4      2347\n",
       "5     38494\n",
       "6      1505\n",
       "7      6888\n",
       "8       371\n",
       "9      6900\n",
       "10     4766\n",
       "11     9836\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"question_family_index\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type\n",
       "action_compare    47534\n",
       "action_query       8393\n",
       "counting          41346\n",
       "existence           874\n",
       "number_compare    14602\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"question_type\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_compare , from: [3 2 9]\n",
      "The tester closed the back Door After closing the refrigerator OR Before closing the front Door?\n",
      "Is it true that the user opened the front Door Preceding closing the Fridge OR After opening the Fridge?\n",
      "The person opened the Fridge Following closing the back Door OR Preceding opening the back Door, correct?\n",
      "The person closed the front Door After closing the back Door AND After opening the back Door?\n",
      "The tester opened the back Door After opening the Fridge AND Before opening the front Door, correct?\n",
      "Is it correct to say that the person closed the Fridge After closing the back Door OR Preceding opening the Fridge?\n",
      "Is it true that the user opened the back Door Preceding closing the front Door AND After closing the refrigerator?\n",
      "The subject closed the back Door After opening the Fridge OR Following opening the Fridge?\n",
      "Does the tester opens the front Door After closing the back Door OR Preceding opening the back Door?\n",
      "Confirm if the tester closed the back Door Before closing the refrigerator AND After opening the front Door.\n",
      "Confirm if the tester closed the front Door Preceding opening the front Door OR Following opening the Fridge.\n",
      "Is it the case that the user opened the refrigerator Preceding opening the back Door AND Before opening the back Door?\n",
      "The tester opened the refrigerator After closing the back Door AND Following closing the Fridge, correct?\n",
      "The subject opened the back Door Preceding closing the refrigerator OR Following opening the front Door?\n",
      "The subject closed the Fridge Preceding closing the back Door OR After closing the front Door?\n",
      "\n",
      "action_query , from: [7 6]\n",
      "What does the user do Following closing the refrigerator and After closing the front Door?\n",
      "What does the subject do Following closing the refrigerator?\n",
      "What does the user do Following closing the refrigerator and After opening the Fridge?\n",
      "What action does the person do After closing the Fridge and After closing the back Door?\n",
      "What action characterizes the person ’ s activity Following closing the Fridge and Before opening the front Door?\n",
      "What action does the tester do After opening the refrigerator and Following closing the Fridge?\n",
      "What action does the subject do Following closing the refrigerator and After opening the back Door?\n",
      "What does the person do Following closing the Fridge and After opening the Fridge?\n",
      "What action characterizes the person ’ s activity Following closing the refrigerator and After opening the Fridge?\n",
      "What action characterizes the tester ’ s activity Following closing the refrigerator and Before closing the back Door?\n",
      "What action characterizes the user ’ s activity Following closing the refrigerator and Before opening the front Door?\n",
      "What does the person do Following closing the refrigerator and After opening the Fridge?\n",
      "What does the person do Following closing the refrigerator and Before opening the back Door?\n",
      "What action does the user do Following closing the refrigerator and After opening the refrigerator?\n",
      "What does the subject do Following closing the refrigerator and Before closing the back Door?\n",
      "\n",
      "counting , from: [5 1 4]\n",
      "How many times can you count the person closed the back Door After closing the Fridge OR Before closing the front Door?\n",
      "What is the number of tester's opening the front Door Preceding closing the Fridge OR After opening the refrigerator?\n",
      "What is the number of tester's opening the refrigerator Following closing the back Door OR Preceding opening the back Door?\n",
      "The person closed the front Door After closing the back Door AND After opening the back Door how many times?\n",
      "Count how many times the subject opened the back Door After opening the refrigerator AND Before opening the front Door?\n",
      "How should one quantify the number of instances that the tester closed the Fridge After closing the back Door OR Preceding opening the refrigerator?\n",
      "How should one quantify the number of instances that the user opened the back Door Preceding closing the front Door AND After closing the Fridge?\n",
      "Count how many times the user closed the front Door?\n",
      "The subject closed the back Door After opening the refrigerator OR Following opening the refrigerator how many times?\n",
      "What is the number of person's opening the front Door After closing the back Door OR Preceding opening the back Door?\n",
      "How many times does the subject closes the back Door Before closing the Fridge AND After opening the front Door?\n",
      "What is the quantity of instances in which the subject closed the front Door Preceding opening the front Door OR Following opening the Fridge?\n",
      "How many times can you count the tester opened the Fridge Preceding opening the back Door AND Before opening the back Door?\n",
      "How should one quantify the number of instances that the subject opened the refrigerator After closing the back Door AND Following closing the Fridge?\n",
      "How should one quantify the number of instances that the subject opened the back Door Preceding closing the Fridge OR Following opening the front Door?\n",
      "\n",
      "existence , from: [0 8]\n",
      "The subject closed the front Door?\n",
      "Confirm if the tester opened the refrigerator?\n",
      "The person opened the refrigerator?\n",
      "The subject opened the front Door?\n",
      "Does the subject opens the front Door?\n",
      "Is it the case that the person opened the front Door?\n",
      "Is it true that the user opened the refrigerator?\n",
      "If the user closed the front Door?\n",
      "Does the tester opens the back Door?\n",
      "Does the tester opens the refrigerator?\n",
      "If the person closed the back Door?\n",
      "Is it the case that the subject closed the Fridge?\n",
      "If the subject opened the back Door?\n",
      "Confirm if the tester opened the front Door?\n",
      "The subject opened the front Door?\n",
      "\n",
      "number_compare , from: [11 10]\n",
      "The person closes the front Door more than opens the refrigerator, correct?\n",
      "Is it the case that the subject closes the front Door more than opens the front Door?\n",
      "Is it the case that the tester closes the front Door more than opens the refrigerator?\n",
      "Confirm if the subject opens the Fridge more than closes the front Door?\n",
      "Is it correct to say that the user opens the front Door more than opens the back Door?\n",
      "The tester closes the back Door more than opens the back Door, correct?\n",
      "Confirm if the tester opens the back Door more than closes the front Door?\n",
      "The tester closes the front Door more than closes the refrigerator?\n",
      "Does the user closes the Fridge more than opens the Fridge?\n",
      "The person opens the front Door more than closes the back Door, correct?\n",
      "Is it correct to say that the user opens the Fridge more than opens the front Door?\n",
      "Is it correct to say that the user opens the refrigerator more than closes the refrigerator?\n",
      "The subject opens the Fridge more than closes the back Door, correct?\n",
      "The person closes the refrigerator more than opens the front Door, correct?\n",
      "Is it correct to say that the tester closes the front Door more than closes the back Door?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the question_type makes sense\n",
    "\n",
    "by_family_index = df_all.groupby(\"question_type\")\n",
    "for family_index, group in by_family_index:\n",
    "    print(family_index, \", from:\", group[\"question_family_index\"].unique())\n",
    "    print(\"\\n\".join(group[\"question\"].iloc[:15]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112749"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"question\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type          5\n",
       "answer                23\n",
       "question           84389\n",
       "question-answer    89735\n",
       "trajectory-str         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"trajectory-str\"] = df_all[\"trajectory\"].apply(lambda x: str(x))\n",
    "df_all[\n",
    "    [\n",
    "        \"question_type\",\n",
    "        \"answer\",\n",
    "        \"question\",\n",
    "        \"question-answer\",\n",
    "        \"trajectory-str\",\n",
    "    ]\n",
    "].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 77)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory_shape = next(iter(get_for(pd_data.iloc[:1]))).trajectory.shape\n",
    "trajectory_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Clean the Table': 0,\n",
       " 'Close the Dishwasher': 1,\n",
       " 'Close the Fridge': 2,\n",
       " 'Close the back Door': 3,\n",
       " 'Close the first Drawer': 4,\n",
       " 'Close the front Door': 5,\n",
       " 'Close the second Drawer': 6,\n",
       " 'Close the third Drawer': 7,\n",
       " 'Drink from the Cup': 8,\n",
       " 'Open the Dishwasher': 9,\n",
       " 'Open the Fridge': 10,\n",
       " 'Open the back Door': 11,\n",
       " 'Open the first Drawer': 12,\n",
       " 'Open the front Door': 13,\n",
       " 'Open the second Drawer': 14,\n",
       " 'Open the third Drawer': 15,\n",
       " 'Toggle the Switch': 16}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_options = pd.Series(\n",
    "    df_all[df_all[\"question_type\"] == \"action_query\"][\"answer\"].unique()\n",
    ").sort_values()\n",
    "multi_option_to_int = {answer: index for index, answer in enumerate(multi_options)}\n",
    "multi_option_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_type_to_answer_type = {\n",
    "    \"existence\": \"binary\",\n",
    "    \"action_compare\": \"binary\",\n",
    "    \"number_compare\": \"binary\",\n",
    "    \"action_query\": \"multi\",\n",
    "    \"counting\": \"count\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_info(answer_type: str) -> DatasetInfo:\n",
    "    base_features = {\n",
    "        # \"sample_id\": Value(\"int32\"),\n",
    "        \"question_id\": Value(\"int32\"),\n",
    "        \"trajectory\": Array2D(dtype=\"float32\", shape=trajectory_shape),\n",
    "        # \"textual_description\": Value(\"string\"),\n",
    "        \"question_type\": Value(\"string\"),\n",
    "        \"question\": Value(\"string\"),\n",
    "        \"answer_type\": Value(\"string\"),\n",
    "        # \"answer\": Value(\"string\"),\n",
    "        # \"options\": Value(\"string\"),  # JSON encoded\n",
    "        # \"correct_option\": Value(\"string\"),\n",
    "    }\n",
    "\n",
    "    match answer_type:\n",
    "        case \"binary\":\n",
    "            answer_features = {\n",
    "                \"answer\": ClassLabel(names=[\"true\", \"false\"], num_classes=2)\n",
    "            }\n",
    "        case \"multi\":\n",
    "            answer_features = {\n",
    "                \"answer\": ClassLabel(\n",
    "                    names=multi_options.to_list(), num_classes=num_multiclass\n",
    "                ),\n",
    "                \"options\": Sequence(Value(\"string\")),\n",
    "            }\n",
    "        case \"count\":\n",
    "            answer_features = {\"answer\": Value(\"uint8\")}\n",
    "        case _:\n",
    "            raise ValueError(f\"Invalid task '{answer_type}'\")\n",
    "\n",
    "    return DatasetInfo(features=Features(base_features | answer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_grouped_df_to_hub(\n",
    "    df_group: DataFrameGroupBy,\n",
    "    split: Literal[\"test\", \"val\", \"train\"],\n",
    "    limit_task: list[str] | None = None,\n",
    "    token: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Takes a grouped DataFrame, feature types, dataset info, and a Hugging Face authentication token,\n",
    "    then pushes each group to the Hugging Face Hub under specified configurations.\n",
    "\n",
    "    :param df_group: Grouped Pandas DataFrame object.\n",
    "    :param feat_type: Feature type for the dataset.\n",
    "    :param info: Information about the dataset.\n",
    "    :param token: Hugging Face authentication token.\n",
    "    \"\"\"\n",
    "    for name, group in df_group:\n",
    "        if limit_task and name not in limit_task:\n",
    "            continue\n",
    "        print(f\"Group Name: {name} of split: {split}\")\n",
    "\n",
    "        answer_type = question_type_to_answer_type[name]\n",
    "\n",
    "        match answer_type:\n",
    "            case \"binary\":\n",
    "                group[\"answer\"] = (group[\"answer\"] == \"Yes\").astype(int)\n",
    "\n",
    "            case \"multi\":\n",
    "                # group[\"answer_index\"] = [\n",
    "                #     multi_option_to_int[ans] for ans in group[\"answer\"]\n",
    "                # ]\n",
    "                group[\"options\"] = group[\"answer\"].apply(\n",
    "                    lambda _: multi_options.to_list()\n",
    "                )\n",
    "\n",
    "            case \"count\":\n",
    "                group[\"answer\"] = group[\"answer\"].astype(int)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid task name: {name}\")\n",
    "\n",
    "        def gen_it():\n",
    "            yield from group.to_dict(orient=\"records\")\n",
    "\n",
    "        match win_len:\n",
    "            case 500:\n",
    "                repo_name = \"dasyd/OppQA-500\"\n",
    "            case 1800:\n",
    "                repo_name = \"dasyd/OppQA\"\n",
    "            case _:\n",
    "                raise ValueError(\n",
    "                    f\"This window length has no dataset attached: {win_len}\"\n",
    "                )\n",
    "\n",
    "        # Create a dataset from the list of dictionaries and push it to the hub\n",
    "        dataset = Dataset.from_generator(\n",
    "            gen_it,\n",
    "            info=make_info(answer_type),\n",
    "        )\n",
    "        dataset.push_to_hub(\n",
    "            repo_name,\n",
    "            config_name=name,\n",
    "            token=token,\n",
    "            split=split,\n",
    "            #     commit_message=f\"[Version Revision] Restructured item shape of {split} split of {name} task dataset\",\n",
    "        )\n",
    "        print(f\"Pushed {name} of split: {split} to Hub {repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group Name: action_compare of split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 37227 examples [01:01, 601.32 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.00s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.85s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.42s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.29s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.27s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.33s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.38s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.04s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.85s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.92s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.42s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.53s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.17s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.43s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.25s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.17s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.49s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.30s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.30s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.97s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.01s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.84s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.63s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.85s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  5.00s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.56s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.17s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.89s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.43s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.40s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.29s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.82s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.44s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.58s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.08s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.79s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.32s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.63s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.75s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.53s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.89s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.91s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 42/42 [08:06<00:00, 11.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_compare of split: train to Hub dasyd/OppQA\n",
      "Group Name: action_query of split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 6161 examples [00:10, 571.85 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.27s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.39s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.45s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.22s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.94s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.60s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.51s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [01:14<00:00, 10.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_query of split: train to Hub dasyd/OppQA\n",
      "Group Name: counting of split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 32215 examples [00:59, 539.40 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.89s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.49s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.56s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.20s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.56s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.52s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.32s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.36s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.96s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.63s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.51s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.04s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.53s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.71s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.25s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.78s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.67s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.47s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.14s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.08s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.47s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.41s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.94s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.03s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.60s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.35s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.48s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.61s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.52s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.07s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.95s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.60s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.49s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.83s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.75s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.72s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 37/37 [06:52<00:00, 11.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed counting of split: train to Hub dasyd/OppQA\n",
      "Group Name: existence of split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 523 examples [00:00, 655.46 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:02<00:00,  2.90s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed existence of split: train to Hub dasyd/OppQA\n",
      "Group Name: number_compare of split: train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 7928 examples [00:13, 582.71 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.54s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.03s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.48s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.96s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.24s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.62s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.58s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.08s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.14s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 9/9 [01:47<00:00, 11.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed number_compare of split: train to Hub dasyd/OppQA\n",
      "Group Name: action_compare of split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4144 examples [00:07, 559.93 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.18s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.74s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.54s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.20s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.55s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [00:51<00:00, 10.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_compare of split: val to Hub dasyd/OppQA\n",
      "Group Name: action_query of split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 620 examples [00:01, 586.16 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.33s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_query of split: val to Hub dasyd/OppQA\n",
      "Group Name: counting of split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3670 examples [00:06, 529.98 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.06s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.82s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.83s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.94s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  4.00s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 5/5 [00:47<00:00,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed counting of split: val to Hub dasyd/OppQA\n",
      "Group Name: existence of split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 68 examples [00:00, 581.98 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  2.74ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed existence of split: val to Hub dasyd/OppQA\n",
      "Group Name: number_compare of split: val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 837 examples [00:01, 646.82 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.15s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed number_compare of split: val to Hub dasyd/OppQA\n",
      "Group Name: action_compare of split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 6163 examples [00:11, 541.74 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.69s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.60s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.57s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.34s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.58s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.64s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.88s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [01:23<00:00, 11.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_compare of split: test to Hub dasyd/OppQA\n",
      "Group Name: action_query of split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1612 examples [00:03, 445.99 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  5.00s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.96s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:20<00:00, 10.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed action_query of split: test to Hub dasyd/OppQA\n",
      "Group Name: counting of split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5461 examples [00:10, 512.99 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.33s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.79s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:03<00:00,  3.99s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.40s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.67s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.11s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.14s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [01:14<00:00, 10.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed counting of split: test to Hub dasyd/OppQA\n",
      "Group Name: existence of split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 283 examples [00:00, 658.47 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:01<00:00,  1.65s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:04<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed existence of split: test to Hub dasyd/OppQA\n",
      "Group Name: number_compare of split: test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 5837 examples [00:10, 531.08 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.94s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.98s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.50s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.73s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:04<00:00,  4.80s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:07<00:00,  7.08s/ba]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:05<00:00,  5.44s/ba]\n",
      "Uploading the dataset shards: 100%|██████████| 7/7 [01:21<00:00, 11.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed number_compare of split: test to Hub dasyd/OppQA\n"
     ]
    }
   ],
   "source": [
    "splits = {\n",
    "    \"train\": train_ind,\n",
    "    \"val\": valid_ind,\n",
    "    \"test\": test_ind,\n",
    "}\n",
    "\n",
    "for name, mask in splits.items():\n",
    "    data = get_for(pd_data[mask])\n",
    "    df = pd.DataFrame(elem._asdict() for elem in data)\n",
    "    push_grouped_df_to_hub(\n",
    "        df.groupby(\"question_type\"),\n",
    "        split=name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to first run:\n",
    "\n",
    "```shell\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if it works (this re-downloads the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 187M/187M [00:08<00:00, 21.7MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 187M/187M [00:09<00:00, 20.2MB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 100%|██████████| 187M/187M [00:08<00:00, 23.0MB/s]\n",
      "Downloading data: 100%|██████████| 187M/187M [00:09<00:00, 19.4MB/s]\n",
      "Downloading data: 100%|██████████| 187M/187M [00:04<00:00, 38.0MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     54\u001b[0m module \u001b[38;5;241m=\u001b[39m TimeQADataModule(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_compare\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m module\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[67], line 32\u001b[0m, in \u001b[0;36mTimeQADataModule.prepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_dataset_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 22\u001b[0m, in \u001b[0;36mTimeQADataModule._load_dataset_split\u001b[0;34m(self, splits)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_dataset_split\u001b[39m(\u001b[38;5;28mself\u001b[39m, splits: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Workaround to overcome the missing hf implementation of only dowloading the split shards\"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTimeQADataModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mKEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplits\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNO_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py:2609\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2608\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2609\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2619\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2620\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[1;32m   1099\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1100\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_generators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msplit_generators_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/packaged_modules/parquet/parquet.py:44\u001b[0m, in \u001b[0;36mParquet._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one data file must be specified, but got data_files=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m dl_manager\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39mextract_on_the_fly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_files, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m     46\u001b[0m     files \u001b[38;5;241m=\u001b[39m data_files\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:434\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/download/download_manager.py:257\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    255\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[0;32m--> 257\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    267\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:494\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    492\u001b[0m     num_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[0;32m--> 494\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_proc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m<\u001b[39m parallel_min_length:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:495\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    492\u001b[0m     num_proc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m iterable):\n\u001b[1;32m    494\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 495\u001b[0m         \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_min_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_min_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m    505\u001b[0m     ]\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m num_proc \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_proc \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m<\u001b[39m parallel_min_length:\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/py_utils.py:528\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    526\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    527\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m--> 528\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_single_map_nested\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[1;32m    532\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/utils/experimental.py:41\u001b[0m, in \u001b[0;36mexperimental.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     38\u001b[0m         (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is experimental and might be subject to breaking changes in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/parallel/parallel.py:34\u001b[0m, in \u001b[0;36mparallel_map\u001b[0;34m(function, iterable, num_proc, batched, batch_size, types, disable_tqdm, desc, single_map_nested_func)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m**Experimental.** Apply a function to iterable elements in parallel, where the implementation uses either\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03mmultiprocessing.Pool or joblib for parallelization.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m        element of `iterable`, and `rank` is used for progress bar.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ParallelBackendConfig\u001b[38;5;241m.\u001b[39mbackend_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_with_multiprocessing_pool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_map_nested_func\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _map_with_joblib(\n\u001b[1;32m     39\u001b[0m     function, iterable, num_proc, batched, batch_size, types, disable_tqdm, desc, single_map_nested_func\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/parallel/parallel.py:69\u001b[0m, in \u001b[0;36m_map_with_multiprocessing_pool\u001b[0;34m(function, iterable, num_proc, batched, batch_size, types, disable_tqdm, desc, single_map_nested_func)\u001b[0m\n\u001b[1;32m     67\u001b[0m     initargs, initializer \u001b[38;5;241m=\u001b[39m (RLock(),), tqdm\u001b[38;5;241m.\u001b[39mset_lock\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(num_proc, initargs\u001b[38;5;241m=\u001b[39minitargs, initializer\u001b[38;5;241m=\u001b[39minitializer) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 69\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_map_nested_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_kwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [obj \u001b[38;5;28;01mfor\u001b[39;00m proc_res \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m proc_res]\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:622\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    620\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 622\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import VerificationMode\n",
    "\n",
    "\n",
    "class TimeQADataModule(LightningDataModule):\n",
    "    KEY = \"dasyd/OppQA\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        task: Literal[\n",
    "            \"existence\", \"action_compare\", \"number_compare\", \"action_query\", \"counting\"\n",
    "        ] = \"action_compare\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.task = task\n",
    "\n",
    "    def _load_dataset_split(self, splits: list[str]):\n",
    "        \"\"\"Workaround to overcome the missing hf implementation of only dowloading the split shards\"\"\"\n",
    "\n",
    "        return load_dataset(\n",
    "            TimeQADataModule.KEY,\n",
    "            self.task,\n",
    "            data_dir=self.task,\n",
    "            data_files={split: f\"{split}-*\" for split in splits},\n",
    "            verification_mode=VerificationMode.NO_CHECKS,\n",
    "            num_proc=len(splits),\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        self._load_dataset_split([\"val\", \"train\", \"test\"])\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.dataset = self._load_dataset_split([\"train\", \"val\"])\n",
    "        elif stage == \"test\":\n",
    "            self.dataset = self._load_dataset_split([\"test\"])\n",
    "\n",
    "        self.dataset = self.dataset.with_format(\"torch\")\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.dataset[\"train\"], batch_size=self.batch_size, shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"val\"], batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.dataset[\"test\"], batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "module = TimeQADataModule(batch_size=4, task=\"action_compare\")\n",
    "module.prepare_data()\n",
    "module.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = module.train_dataloader()\n",
    "batch = next(iter(loader))\n",
    "\n",
    "# list(batch.keys())\n",
    "batch[\"trajectory\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
